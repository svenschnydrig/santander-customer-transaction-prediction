{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sum Subset - Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data.dataset import random_split\n",
    "from math import ceil\n",
    "\n",
    "train_data = pd.read_csv(\"/Users/svenschnydrig/My Drive/Data Science Project - Team D/data/fe1-sum/train_fe1_sum.csv\")\n",
    "one = train_data.iloc[:, :202]\n",
    "two = train_data.iloc[:, 402:602]\n",
    "train_data = pd.concat([one, two,], axis=1)\n",
    "y = train_data[\"target\"]\n",
    "X = train_data.drop([\"ID_code\", \"target\"], axis=1)\n",
    "X_tensor = torch.tensor(X.values, dtype=torch.float32)\n",
    "y_tensor = torch.tensor(y.values, dtype=torch.float32)\n",
    "ds = TensorDataset(X_tensor, y_tensor)\n",
    "train_ds, val_ds = random_split(ds, [int(0.8*len(ds)), ceil(0.2*len(ds))])\n",
    "#for final training we use all data\n",
    "train_ds = TensorDataset(X_tensor, y_tensor)\n",
    "\n",
    "\n",
    "test_data = pd.read_csv(\"/Users/svenschnydrig/My Drive/Data Science Project - Team D/data/fe1-sum/test_fe1_sum.csv\")\n",
    "one = test_data.iloc[:, :201]\n",
    "two = test_data.iloc[:, 401:601]\n",
    "test_data = pd.concat([one, two], axis=1)\n",
    "test_ids = test_data[\"ID_code\"]\n",
    "X = test_data.drop([\"ID_code\"], axis=1)\n",
    "X_tensor = torch.tensor(X.values, dtype=torch.float32)\n",
    "y_tensor = torch.tensor(y.values, dtype=torch.float32)\n",
    "test_ds = TensorDataset(X_tensor, y_tensor)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sum Subset - Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VALIDATION ROC: 0.5232673153496404\n",
      "VALIDATION ROC: 0.9132204311906339\n",
      "VALIDATION ROC: 0.9185611574536371\n",
      "VALIDATION ROC: 0.920393405182126\n",
      "VALIDATION ROC: 0.9210814661593437\n",
      "VALIDATION ROC: 0.9218039794378029\n",
      "VALIDATION ROC: 0.9232080377601676\n",
      "VALIDATION ROC: 0.9231081929166528\n",
      "VALIDATION ROC: 0.9238321263344237\n",
      "VALIDATION ROC: 0.9240852348401191\n",
      "VALIDATION ROC: 0.9246959399936563\n",
      "VALIDATION ROC: 0.9241963711834547\n",
      "VALIDATION ROC: 0.9256161450180549\n",
      "VALIDATION ROC: 0.9259245937073669\n",
      "VALIDATION ROC: 0.9255100766228508\n",
      "VALIDATION ROC: 0.9259018714783809\n",
      "VALIDATION ROC: 0.9261158078080638\n",
      "VALIDATION ROC: 0.926150580336797\n",
      "VALIDATION ROC: 0.9261970620533839\n",
      "VALIDATION ROC: 0.9259566338602186\n",
      "torch.Size([1000, 400])\n",
      "torch.Size([1000, 400])\n",
      "torch.Size([1000, 400])\n",
      "torch.Size([1000, 400])\n",
      "torch.Size([1000, 400])\n",
      "torch.Size([1000, 400])\n",
      "torch.Size([1000, 400])\n",
      "torch.Size([1000, 400])\n",
      "torch.Size([1000, 400])\n",
      "torch.Size([1000, 400])\n",
      "torch.Size([1000, 400])\n",
      "torch.Size([1000, 400])\n",
      "torch.Size([1000, 400])\n",
      "torch.Size([1000, 400])\n",
      "torch.Size([1000, 400])\n",
      "torch.Size([1000, 400])\n",
      "torch.Size([1000, 400])\n",
      "torch.Size([1000, 400])\n",
      "torch.Size([1000, 400])\n",
      "torch.Size([1000, 400])\n",
      "torch.Size([1000, 400])\n",
      "torch.Size([1000, 400])\n",
      "torch.Size([1000, 400])\n",
      "torch.Size([1000, 400])\n",
      "torch.Size([1000, 400])\n",
      "torch.Size([1000, 400])\n",
      "torch.Size([1000, 400])\n",
      "torch.Size([1000, 400])\n",
      "torch.Size([1000, 400])\n",
      "torch.Size([1000, 400])\n",
      "torch.Size([1000, 400])\n",
      "torch.Size([1000, 400])\n",
      "torch.Size([1000, 400])\n",
      "torch.Size([1000, 400])\n",
      "torch.Size([1000, 400])\n",
      "torch.Size([1000, 400])\n",
      "torch.Size([1000, 400])\n",
      "torch.Size([1000, 400])\n",
      "torch.Size([1000, 400])\n",
      "torch.Size([1000, 400])\n",
      "torch.Size([1000, 400])\n",
      "torch.Size([1000, 400])\n",
      "torch.Size([1000, 400])\n",
      "torch.Size([1000, 400])\n",
      "torch.Size([1000, 400])\n",
      "torch.Size([1000, 400])\n",
      "torch.Size([1000, 400])\n",
      "torch.Size([1000, 400])\n",
      "torch.Size([1000, 400])\n",
      "torch.Size([1000, 400])\n",
      "torch.Size([1000, 400])\n",
      "torch.Size([1000, 400])\n",
      "torch.Size([1000, 400])\n",
      "torch.Size([1000, 400])\n",
      "torch.Size([1000, 400])\n",
      "torch.Size([1000, 400])\n",
      "torch.Size([1000, 400])\n",
      "torch.Size([1000, 400])\n",
      "torch.Size([1000, 400])\n",
      "torch.Size([1000, 400])\n",
      "torch.Size([1000, 400])\n",
      "torch.Size([1000, 400])\n",
      "torch.Size([1000, 400])\n",
      "torch.Size([1000, 400])\n",
      "torch.Size([1000, 400])\n",
      "torch.Size([1000, 400])\n",
      "torch.Size([1000, 400])\n",
      "torch.Size([1000, 400])\n",
      "torch.Size([1000, 400])\n",
      "torch.Size([1000, 400])\n",
      "torch.Size([1000, 400])\n",
      "torch.Size([1000, 400])\n",
      "torch.Size([1000, 400])\n",
      "torch.Size([1000, 400])\n",
      "torch.Size([1000, 400])\n",
      "torch.Size([1000, 400])\n",
      "torch.Size([1000, 400])\n",
      "torch.Size([1000, 400])\n",
      "torch.Size([1000, 400])\n",
      "torch.Size([1000, 400])\n",
      "torch.Size([1000, 400])\n",
      "torch.Size([1000, 400])\n",
      "torch.Size([1000, 400])\n",
      "torch.Size([1000, 400])\n",
      "torch.Size([1000, 400])\n",
      "torch.Size([1000, 400])\n",
      "torch.Size([1000, 400])\n",
      "torch.Size([1000, 400])\n",
      "torch.Size([1000, 400])\n",
      "torch.Size([1000, 400])\n",
      "torch.Size([1000, 400])\n",
      "torch.Size([1000, 400])\n",
      "torch.Size([1000, 400])\n",
      "torch.Size([1000, 400])\n",
      "torch.Size([1000, 400])\n",
      "torch.Size([1000, 400])\n",
      "torch.Size([1000, 400])\n",
      "torch.Size([1000, 400])\n",
      "torch.Size([1000, 400])\n",
      "torch.Size([1000, 400])\n",
      "torch.Size([1000, 400])\n",
      "torch.Size([1000, 400])\n",
      "torch.Size([1000, 400])\n",
      "torch.Size([1000, 400])\n",
      "torch.Size([1000, 400])\n",
      "torch.Size([1000, 400])\n",
      "torch.Size([1000, 400])\n",
      "torch.Size([1000, 400])\n",
      "torch.Size([1000, 400])\n",
      "torch.Size([1000, 400])\n",
      "torch.Size([1000, 400])\n",
      "torch.Size([1000, 400])\n",
      "torch.Size([1000, 400])\n",
      "torch.Size([1000, 400])\n",
      "torch.Size([1000, 400])\n",
      "torch.Size([1000, 400])\n",
      "torch.Size([1000, 400])\n",
      "torch.Size([1000, 400])\n",
      "torch.Size([1000, 400])\n",
      "torch.Size([1000, 400])\n",
      "torch.Size([1000, 400])\n",
      "torch.Size([1000, 400])\n",
      "torch.Size([1000, 400])\n",
      "torch.Size([1000, 400])\n",
      "torch.Size([1000, 400])\n",
      "torch.Size([1000, 400])\n",
      "torch.Size([1000, 400])\n",
      "torch.Size([1000, 400])\n",
      "torch.Size([1000, 400])\n",
      "torch.Size([1000, 400])\n",
      "torch.Size([1000, 400])\n",
      "torch.Size([1000, 400])\n",
      "torch.Size([1000, 400])\n",
      "torch.Size([1000, 400])\n",
      "torch.Size([1000, 400])\n",
      "torch.Size([1000, 400])\n",
      "torch.Size([1000, 400])\n",
      "torch.Size([1000, 400])\n",
      "torch.Size([1000, 400])\n",
      "torch.Size([1000, 400])\n",
      "torch.Size([1000, 400])\n",
      "torch.Size([1000, 400])\n",
      "torch.Size([1000, 400])\n",
      "torch.Size([1000, 400])\n",
      "torch.Size([1000, 400])\n",
      "torch.Size([1000, 400])\n",
      "torch.Size([1000, 400])\n",
      "torch.Size([1000, 400])\n",
      "torch.Size([1000, 400])\n",
      "torch.Size([1000, 400])\n",
      "torch.Size([1000, 400])\n",
      "torch.Size([1000, 400])\n",
      "torch.Size([1000, 400])\n",
      "torch.Size([1000, 400])\n",
      "torch.Size([1000, 400])\n",
      "torch.Size([1000, 400])\n",
      "torch.Size([1000, 400])\n",
      "torch.Size([1000, 400])\n",
      "torch.Size([1000, 400])\n",
      "torch.Size([1000, 400])\n",
      "torch.Size([1000, 400])\n",
      "torch.Size([1000, 400])\n",
      "torch.Size([1000, 400])\n",
      "torch.Size([1000, 400])\n",
      "torch.Size([1000, 400])\n",
      "torch.Size([1000, 400])\n",
      "torch.Size([1000, 400])\n",
      "torch.Size([1000, 400])\n",
      "torch.Size([1000, 400])\n",
      "torch.Size([1000, 400])\n",
      "torch.Size([1000, 400])\n",
      "torch.Size([1000, 400])\n",
      "torch.Size([1000, 400])\n",
      "torch.Size([1000, 400])\n",
      "torch.Size([1000, 400])\n",
      "torch.Size([1000, 400])\n",
      "torch.Size([1000, 400])\n",
      "torch.Size([1000, 400])\n",
      "torch.Size([1000, 400])\n",
      "torch.Size([1000, 400])\n",
      "torch.Size([1000, 400])\n",
      "torch.Size([1000, 400])\n",
      "torch.Size([1000, 400])\n",
      "torch.Size([1000, 400])\n",
      "torch.Size([1000, 400])\n",
      "torch.Size([1000, 400])\n",
      "torch.Size([1000, 400])\n",
      "torch.Size([1000, 400])\n",
      "torch.Size([1000, 400])\n",
      "torch.Size([1000, 400])\n",
      "torch.Size([1000, 400])\n",
      "torch.Size([1000, 400])\n",
      "torch.Size([1000, 400])\n",
      "torch.Size([1000, 400])\n",
      "torch.Size([1000, 400])\n",
      "torch.Size([1000, 400])\n",
      "torch.Size([1000, 400])\n",
      "torch.Size([1000, 400])\n",
      "torch.Size([1000, 400])\n",
      "torch.Size([1000, 400])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from sklearn import metrics\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "class NN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_dim):\n",
    "        super(NN, self).__init__()\n",
    "        self.bn = nn.BatchNorm1d(input_size)\n",
    "        self.fc1 = nn.Linear(2, hidden_dim)\n",
    "        self.fc2 = nn.Linear(input_size//2*hidden_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        N = x.shape[0]\n",
    "        x = self.bn(x)\n",
    "        og_features = x[:, :200].unsqueeze(2) # (N, 200, 1)\n",
    "        sum = x[:, 200:].unsqueeze(2) # (N, 200, 1)\n",
    "        x = torch.cat([og_features, sum], dim = 2) # (N, 200, 2)\n",
    "        x = F.relu(self.fc1(x)).reshape(N, -1) # (N, 200*hidden_dim)\n",
    "        return torch.sigmoid(self.fc2(x)).view(-1)\n",
    "\n",
    "DEVICE = torch.device(\"mps\")\n",
    "\n",
    "model = NN(input_size=400, hidden_dim=200).to(DEVICE)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "loss_fn = nn.BCELoss()\n",
    "train_loader = DataLoader(train_ds, batch_size=1000, shuffle=True)\n",
    "val_loader = DataLoader(val_ds, batch_size=1000)\n",
    "test_loader = DataLoader(test_ds, batch_size=1000)\n",
    "\n",
    "def get_predictions(loader, model, device):\n",
    "    model.eval()\n",
    "    saved_preds = []\n",
    "    true_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x,y in loader:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            scores = model(x)\n",
    "            saved_preds += scores.tolist()\n",
    "            true_labels += y.tolist()\n",
    "\n",
    "    model.train()\n",
    "    return saved_preds, true_labels\n",
    "\n",
    "for epoch in range(20):\n",
    "    probabilities, true = get_predictions(val_loader, model, device=DEVICE)\n",
    "    print(f\"VALIDATION ROC: {metrics.roc_auc_score(true, probabilities)}\")\n",
    "\n",
    "    for batch_idx, (data, targets) in enumerate(train_loader):\n",
    "        data = data.to(DEVICE)\n",
    "        targets = targets.to(DEVICE)\n",
    "\n",
    "        # forward\n",
    "        scores = model(data)\n",
    "        loss = loss_fn(scores, targets)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "def get_submission(model, loader, test_ids, device):\n",
    "    all_preds = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for x,y in loader:\n",
    "            print(x.shape)\n",
    "            x = x.to(device)\n",
    "            score = model(x)\n",
    "            prediction = score.float()\n",
    "            all_preds += prediction.tolist()\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        \"ID_code\" : test_ids.values,\n",
    "        \"target\" : np.array(all_preds)\n",
    "    })\n",
    "\n",
    "    df.to_csv(\"/Users/svenschnydrig/My Drive/Data Science Project - Team D/submission/various_submissions/fe1_sum_subset.csv\", index=False)\n",
    "\n",
    "get_submission(model, test_loader, test_ids, DEVICE)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary + Sum Subset - Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data.dataset import random_split\n",
    "from math import ceil\n",
    "\n",
    "train_data = pd.read_csv(\"/Users/svenschnydrig/My Drive/Data Science Project - Team D/data/fe1-sum/train_fe1_sum.csv\")\n",
    "one = train_data.iloc[:, :202]\n",
    "two = train_data.iloc[:, 402:602]\n",
    "train_data = pd.read_csv(\"/Users/svenschnydrig/My Drive/Data Science Project - Team D/data/fe1-binary/train_fe1_binary.csv\")\n",
    "three = train_data.iloc[:, 202:]\n",
    "train_data = pd.concat([one, two, three], axis=1)\n",
    "y = train_data[\"target\"]\n",
    "X = train_data.drop([\"ID_code\", \"target\"], axis=1)\n",
    "X_tensor = torch.tensor(X.values, dtype=torch.float32)\n",
    "y_tensor = torch.tensor(y.values, dtype=torch.float32)\n",
    "ds = TensorDataset(X_tensor, y_tensor)\n",
    "train_ds, val_ds = random_split(ds, [int(0.99*len(ds)), ceil(0.01*len(ds))])\n",
    "#for final training we use the whole dataset\n",
    "train_ds = TensorDataset(X_tensor, y_tensor)\n",
    "\n",
    "test_data = pd.read_csv(\"/Users/svenschnydrig/My Drive/Data Science Project - Team D/data/fe1-sum/test_fe1_sum.csv\")\n",
    "one = test_data.iloc[:, :201]\n",
    "two = test_data.iloc[:, 401:601]\n",
    "test_data = pd.read_csv(\"/Users/svenschnydrig/My Drive/Data Science Project - Team D/data/fe1-binary/test_fe1_binary.csv\")\n",
    "three = test_data.iloc[:, 201:]\n",
    "test_data = pd.concat([one, two, three], axis=1)\n",
    "test_ids = test_data[\"ID_code\"]\n",
    "X = test_data.drop([\"ID_code\"], axis=1)\n",
    "X_tensor = torch.tensor(X.values, dtype=torch.float32)\n",
    "y_tensor = torch.tensor(y.values, dtype=torch.float32)\n",
    "test_ds = TensorDataset(X_tensor, y_tensor)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary + Sum Subset - Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VALIDATION ROC: 0.545109361475004\n",
      "VALIDATION ROC: 0.9198919244798796\n",
      "VALIDATION ROC: 0.9198310836036604\n",
      "VALIDATION ROC: 0.9217696951595552\n",
      "VALIDATION ROC: 0.9242697020732912\n",
      "VALIDATION ROC: 0.9257243521138057\n",
      "VALIDATION ROC: 0.9255639534401368\n",
      "VALIDATION ROC: 0.9257824274956511\n",
      "VALIDATION ROC: 0.9245241275556626\n",
      "VALIDATION ROC: 0.9250219165429109\n",
      "VALIDATION ROC: 0.9243720253651144\n",
      "VALIDATION ROC: 0.9262608580222843\n",
      "VALIDATION ROC: 0.9253731343283582\n",
      "VALIDATION ROC: 0.9254865195976758\n",
      "VALIDATION ROC: 0.9264489116396892\n",
      "VALIDATION ROC: 0.9251408328009756\n",
      "VALIDATION ROC: 0.925256983564667\n",
      "VALIDATION ROC: 0.9257990204618929\n",
      "VALIDATION ROC: 0.9260368529780225\n",
      "VALIDATION ROC: 0.9259207022143314\n",
      "torch.Size([1000, 600])\n",
      "torch.Size([1000, 600])\n",
      "torch.Size([1000, 600])\n",
      "torch.Size([1000, 600])\n",
      "torch.Size([1000, 600])\n",
      "torch.Size([1000, 600])\n",
      "torch.Size([1000, 600])\n",
      "torch.Size([1000, 600])\n",
      "torch.Size([1000, 600])\n",
      "torch.Size([1000, 600])\n",
      "torch.Size([1000, 600])\n",
      "torch.Size([1000, 600])\n",
      "torch.Size([1000, 600])\n",
      "torch.Size([1000, 600])\n",
      "torch.Size([1000, 600])\n",
      "torch.Size([1000, 600])\n",
      "torch.Size([1000, 600])\n",
      "torch.Size([1000, 600])\n",
      "torch.Size([1000, 600])\n",
      "torch.Size([1000, 600])\n",
      "torch.Size([1000, 600])\n",
      "torch.Size([1000, 600])\n",
      "torch.Size([1000, 600])\n",
      "torch.Size([1000, 600])\n",
      "torch.Size([1000, 600])\n",
      "torch.Size([1000, 600])\n",
      "torch.Size([1000, 600])\n",
      "torch.Size([1000, 600])\n",
      "torch.Size([1000, 600])\n",
      "torch.Size([1000, 600])\n",
      "torch.Size([1000, 600])\n",
      "torch.Size([1000, 600])\n",
      "torch.Size([1000, 600])\n",
      "torch.Size([1000, 600])\n",
      "torch.Size([1000, 600])\n",
      "torch.Size([1000, 600])\n",
      "torch.Size([1000, 600])\n",
      "torch.Size([1000, 600])\n",
      "torch.Size([1000, 600])\n",
      "torch.Size([1000, 600])\n",
      "torch.Size([1000, 600])\n",
      "torch.Size([1000, 600])\n",
      "torch.Size([1000, 600])\n",
      "torch.Size([1000, 600])\n",
      "torch.Size([1000, 600])\n",
      "torch.Size([1000, 600])\n",
      "torch.Size([1000, 600])\n",
      "torch.Size([1000, 600])\n",
      "torch.Size([1000, 600])\n",
      "torch.Size([1000, 600])\n",
      "torch.Size([1000, 600])\n",
      "torch.Size([1000, 600])\n",
      "torch.Size([1000, 600])\n",
      "torch.Size([1000, 600])\n",
      "torch.Size([1000, 600])\n",
      "torch.Size([1000, 600])\n",
      "torch.Size([1000, 600])\n",
      "torch.Size([1000, 600])\n",
      "torch.Size([1000, 600])\n",
      "torch.Size([1000, 600])\n",
      "torch.Size([1000, 600])\n",
      "torch.Size([1000, 600])\n",
      "torch.Size([1000, 600])\n",
      "torch.Size([1000, 600])\n",
      "torch.Size([1000, 600])\n",
      "torch.Size([1000, 600])\n",
      "torch.Size([1000, 600])\n",
      "torch.Size([1000, 600])\n",
      "torch.Size([1000, 600])\n",
      "torch.Size([1000, 600])\n",
      "torch.Size([1000, 600])\n",
      "torch.Size([1000, 600])\n",
      "torch.Size([1000, 600])\n",
      "torch.Size([1000, 600])\n",
      "torch.Size([1000, 600])\n",
      "torch.Size([1000, 600])\n",
      "torch.Size([1000, 600])\n",
      "torch.Size([1000, 600])\n",
      "torch.Size([1000, 600])\n",
      "torch.Size([1000, 600])\n",
      "torch.Size([1000, 600])\n",
      "torch.Size([1000, 600])\n",
      "torch.Size([1000, 600])\n",
      "torch.Size([1000, 600])\n",
      "torch.Size([1000, 600])\n",
      "torch.Size([1000, 600])\n",
      "torch.Size([1000, 600])\n",
      "torch.Size([1000, 600])\n",
      "torch.Size([1000, 600])\n",
      "torch.Size([1000, 600])\n",
      "torch.Size([1000, 600])\n",
      "torch.Size([1000, 600])\n",
      "torch.Size([1000, 600])\n",
      "torch.Size([1000, 600])\n",
      "torch.Size([1000, 600])\n",
      "torch.Size([1000, 600])\n",
      "torch.Size([1000, 600])\n",
      "torch.Size([1000, 600])\n",
      "torch.Size([1000, 600])\n",
      "torch.Size([1000, 600])\n",
      "torch.Size([1000, 600])\n",
      "torch.Size([1000, 600])\n",
      "torch.Size([1000, 600])\n",
      "torch.Size([1000, 600])\n",
      "torch.Size([1000, 600])\n",
      "torch.Size([1000, 600])\n",
      "torch.Size([1000, 600])\n",
      "torch.Size([1000, 600])\n",
      "torch.Size([1000, 600])\n",
      "torch.Size([1000, 600])\n",
      "torch.Size([1000, 600])\n",
      "torch.Size([1000, 600])\n",
      "torch.Size([1000, 600])\n",
      "torch.Size([1000, 600])\n",
      "torch.Size([1000, 600])\n",
      "torch.Size([1000, 600])\n",
      "torch.Size([1000, 600])\n",
      "torch.Size([1000, 600])\n",
      "torch.Size([1000, 600])\n",
      "torch.Size([1000, 600])\n",
      "torch.Size([1000, 600])\n",
      "torch.Size([1000, 600])\n",
      "torch.Size([1000, 600])\n",
      "torch.Size([1000, 600])\n",
      "torch.Size([1000, 600])\n",
      "torch.Size([1000, 600])\n",
      "torch.Size([1000, 600])\n",
      "torch.Size([1000, 600])\n",
      "torch.Size([1000, 600])\n",
      "torch.Size([1000, 600])\n",
      "torch.Size([1000, 600])\n",
      "torch.Size([1000, 600])\n",
      "torch.Size([1000, 600])\n",
      "torch.Size([1000, 600])\n",
      "torch.Size([1000, 600])\n",
      "torch.Size([1000, 600])\n",
      "torch.Size([1000, 600])\n",
      "torch.Size([1000, 600])\n",
      "torch.Size([1000, 600])\n",
      "torch.Size([1000, 600])\n",
      "torch.Size([1000, 600])\n",
      "torch.Size([1000, 600])\n",
      "torch.Size([1000, 600])\n",
      "torch.Size([1000, 600])\n",
      "torch.Size([1000, 600])\n",
      "torch.Size([1000, 600])\n",
      "torch.Size([1000, 600])\n",
      "torch.Size([1000, 600])\n",
      "torch.Size([1000, 600])\n",
      "torch.Size([1000, 600])\n",
      "torch.Size([1000, 600])\n",
      "torch.Size([1000, 600])\n",
      "torch.Size([1000, 600])\n",
      "torch.Size([1000, 600])\n",
      "torch.Size([1000, 600])\n",
      "torch.Size([1000, 600])\n",
      "torch.Size([1000, 600])\n",
      "torch.Size([1000, 600])\n",
      "torch.Size([1000, 600])\n",
      "torch.Size([1000, 600])\n",
      "torch.Size([1000, 600])\n",
      "torch.Size([1000, 600])\n",
      "torch.Size([1000, 600])\n",
      "torch.Size([1000, 600])\n",
      "torch.Size([1000, 600])\n",
      "torch.Size([1000, 600])\n",
      "torch.Size([1000, 600])\n",
      "torch.Size([1000, 600])\n",
      "torch.Size([1000, 600])\n",
      "torch.Size([1000, 600])\n",
      "torch.Size([1000, 600])\n",
      "torch.Size([1000, 600])\n",
      "torch.Size([1000, 600])\n",
      "torch.Size([1000, 600])\n",
      "torch.Size([1000, 600])\n",
      "torch.Size([1000, 600])\n",
      "torch.Size([1000, 600])\n",
      "torch.Size([1000, 600])\n",
      "torch.Size([1000, 600])\n",
      "torch.Size([1000, 600])\n",
      "torch.Size([1000, 600])\n",
      "torch.Size([1000, 600])\n",
      "torch.Size([1000, 600])\n",
      "torch.Size([1000, 600])\n",
      "torch.Size([1000, 600])\n",
      "torch.Size([1000, 600])\n",
      "torch.Size([1000, 600])\n",
      "torch.Size([1000, 600])\n",
      "torch.Size([1000, 600])\n",
      "torch.Size([1000, 600])\n",
      "torch.Size([1000, 600])\n",
      "torch.Size([1000, 600])\n",
      "torch.Size([1000, 600])\n",
      "torch.Size([1000, 600])\n",
      "torch.Size([1000, 600])\n",
      "torch.Size([1000, 600])\n",
      "torch.Size([1000, 600])\n",
      "torch.Size([1000, 600])\n",
      "torch.Size([1000, 600])\n",
      "torch.Size([1000, 600])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from sklearn import metrics\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "class NN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_dim):\n",
    "        super(NN, self).__init__()\n",
    "        self.bn = nn.BatchNorm1d(input_size)\n",
    "        self.fc1 = nn.Linear(3, hidden_dim)\n",
    "        self.fc2 = nn.Linear(input_size//3*hidden_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        N = x.shape[0]\n",
    "        x = self.bn(x)\n",
    "        orig_features = x[:, :200].unsqueeze(2) # (N, 200, 1)\n",
    "        sum = x[:, 200:400].unsqueeze(2) # (N, 200, 1)\n",
    "        binary = x[:, 400:].unsqueeze(2) # (N, 200, 1)\n",
    "        x = torch.cat([orig_features, sum, binary], dim = 2) # (N, 200, 2)\n",
    "        x = F.relu(self.fc1(x)).reshape(N, -1) # (N, 200*hidden_dim)\n",
    "        return torch.sigmoid(self.fc2(x)).view(-1)\n",
    "\n",
    "DEVICE = torch.device(\"mps\")\n",
    "\n",
    "model = NN(input_size=600, hidden_dim=200).to(DEVICE)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "loss_fn = nn.BCELoss()\n",
    "train_loader = DataLoader(train_ds, batch_size=1000, shuffle=True)\n",
    "val_loader = DataLoader(val_ds, batch_size=1000)\n",
    "test_loader = DataLoader(test_ds, batch_size=1000)\n",
    "\n",
    "def get_predictions(loader, model, device):\n",
    "    model.eval()\n",
    "    saved_preds = []\n",
    "    true_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x,y in loader:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            scores = model(x)\n",
    "            saved_preds += scores.tolist()\n",
    "            true_labels += y.tolist()\n",
    "\n",
    "    model.train()\n",
    "    return saved_preds, true_labels\n",
    "\n",
    "for epoch in range(20):\n",
    "    probabilities, true = get_predictions(val_loader, model, device=DEVICE)\n",
    "    print(f\"VALIDATION ROC: {metrics.roc_auc_score(true, probabilities)}\")\n",
    "\n",
    "    for batch_idx, (data, targets) in enumerate(train_loader):\n",
    "        data = data.to(DEVICE)\n",
    "        targets = targets.to(DEVICE)\n",
    "\n",
    "        # forward\n",
    "        scores = model(data)\n",
    "        loss = loss_fn(scores, targets)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "def get_submission(model, loader, test_ids, device):\n",
    "    all_preds = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for x,y in loader:\n",
    "            print(x.shape)\n",
    "            x = x.to(device)\n",
    "            score = model(x)\n",
    "            prediction = score.float()\n",
    "            all_preds += prediction.tolist()\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        \"ID_code\" : test_ids.values,\n",
    "        \"target\" : np.array(all_preds)\n",
    "    })\n",
    "\n",
    "    df.to_csv(\"/Users/svenschnydrig/My Drive/Data Science Project - Team D/submission/various_submissions/fe1_binary_sum.csv\", index=False)\n",
    "\n",
    "get_submission(model, test_loader, test_ids, DEVICE)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary + Sum + VC - Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data.dataset import random_split\n",
    "from math import ceil\n",
    "\n",
    "train_data = pd.read_csv(\"/Users/svenschnydrig/My Drive/Data Science Project - Team D/data/fe1-sum/train_fe1_sum.csv\")\n",
    "one = train_data.iloc[:, :202]\n",
    "four = train_data.iloc[:, 202:402]\n",
    "two = train_data.iloc[:, 402:602]\n",
    "train_data = pd.read_csv(\"/Users/svenschnydrig/My Drive/Data Science Project - Team D/data/fe1-binary/train_fe1_binary.csv\")\n",
    "three = train_data.iloc[:, 202:]\n",
    "train_data = pd.concat([one, four, two, three], axis=1)\n",
    "y = train_data[\"target\"]\n",
    "X = train_data.drop([\"ID_code\", \"target\"], axis=1)\n",
    "X_tensor = torch.tensor(X.values, dtype=torch.float32)\n",
    "y_tensor = torch.tensor(y.values, dtype=torch.float32)\n",
    "ds = TensorDataset(X_tensor, y_tensor)\n",
    "train_ds, val_ds = random_split(ds, [int(0.99*len(ds)), ceil(0.01*len(ds))])\n",
    "\n",
    "test_data = pd.read_csv(\"/Users/svenschnydrig/My Drive/Data Science Project - Team D/data/fe1-sum/test_fe1_sum.csv\")\n",
    "one = test_data.iloc[:, :201]\n",
    "four = train_data.iloc[:, 201:401]\n",
    "two = test_data.iloc[:, 401:601]\n",
    "test_data = pd.read_csv(\"/Users/svenschnydrig/My Drive/Data Science Project - Team D/data/fe1-binary/test_fe1_binary.csv\")\n",
    "three = test_data.iloc[:, 201:]\n",
    "test_data = pd.concat([one, four, two, three], axis=1)\n",
    "test_ids = test_data[\"ID_code\"]\n",
    "X = test_data.drop([\"ID_code\"], axis=1)\n",
    "X_tensor = torch.tensor(X.values, dtype=torch.float32)\n",
    "y_tensor = torch.tensor(y.values, dtype=torch.float32)\n",
    "test_ds = TensorDataset(X_tensor, y_tensor)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary + Sum + VC - Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VALIDATION ROC: 0.5312058481857856\n",
      "VALIDATION ROC: 0.9122772132159789\n",
      "VALIDATION ROC: 0.9197836757333779\n",
      "VALIDATION ROC: 0.9221646031961989\n",
      "VALIDATION ROC: 0.9251943988027336\n",
      "VALIDATION ROC: 0.9252519596864501\n",
      "VALIDATION ROC: 0.9259217590606064\n",
      "VALIDATION ROC: 0.929935322497933\n",
      "VALIDATION ROC: 0.931858379294827\n",
      "VALIDATION ROC: 0.9286846814790007\n",
      "VALIDATION ROC: 0.9307464076775753\n",
      "VALIDATION ROC: 0.9319028581595169\n",
      "VALIDATION ROC: 0.9336453830938453\n",
      "VALIDATION ROC: 0.9324784669966824\n",
      "VALIDATION ROC: 0.9322665382884533\n",
      "VALIDATION ROC: 0.9329101736245566\n",
      "VALIDATION ROC: 0.9336166026519869\n",
      "VALIDATION ROC: 0.932624985609779\n",
      "VALIDATION ROC: 0.9347573547110967\n",
      "VALIDATION ROC: 0.933880859436322\n",
      "torch.Size([1000, 800])\n",
      "torch.Size([1000, 800])\n",
      "torch.Size([1000, 800])\n",
      "torch.Size([1000, 800])\n",
      "torch.Size([1000, 800])\n",
      "torch.Size([1000, 800])\n",
      "torch.Size([1000, 800])\n",
      "torch.Size([1000, 800])\n",
      "torch.Size([1000, 800])\n",
      "torch.Size([1000, 800])\n",
      "torch.Size([1000, 800])\n",
      "torch.Size([1000, 800])\n",
      "torch.Size([1000, 800])\n",
      "torch.Size([1000, 800])\n",
      "torch.Size([1000, 800])\n",
      "torch.Size([1000, 800])\n",
      "torch.Size([1000, 800])\n",
      "torch.Size([1000, 800])\n",
      "torch.Size([1000, 800])\n",
      "torch.Size([1000, 800])\n",
      "torch.Size([1000, 800])\n",
      "torch.Size([1000, 800])\n",
      "torch.Size([1000, 800])\n",
      "torch.Size([1000, 800])\n",
      "torch.Size([1000, 800])\n",
      "torch.Size([1000, 800])\n",
      "torch.Size([1000, 800])\n",
      "torch.Size([1000, 800])\n",
      "torch.Size([1000, 800])\n",
      "torch.Size([1000, 800])\n",
      "torch.Size([1000, 800])\n",
      "torch.Size([1000, 800])\n",
      "torch.Size([1000, 800])\n",
      "torch.Size([1000, 800])\n",
      "torch.Size([1000, 800])\n",
      "torch.Size([1000, 800])\n",
      "torch.Size([1000, 800])\n",
      "torch.Size([1000, 800])\n",
      "torch.Size([1000, 800])\n",
      "torch.Size([1000, 800])\n",
      "torch.Size([1000, 800])\n",
      "torch.Size([1000, 800])\n",
      "torch.Size([1000, 800])\n",
      "torch.Size([1000, 800])\n",
      "torch.Size([1000, 800])\n",
      "torch.Size([1000, 800])\n",
      "torch.Size([1000, 800])\n",
      "torch.Size([1000, 800])\n",
      "torch.Size([1000, 800])\n",
      "torch.Size([1000, 800])\n",
      "torch.Size([1000, 800])\n",
      "torch.Size([1000, 800])\n",
      "torch.Size([1000, 800])\n",
      "torch.Size([1000, 800])\n",
      "torch.Size([1000, 800])\n",
      "torch.Size([1000, 800])\n",
      "torch.Size([1000, 800])\n",
      "torch.Size([1000, 800])\n",
      "torch.Size([1000, 800])\n",
      "torch.Size([1000, 800])\n",
      "torch.Size([1000, 800])\n",
      "torch.Size([1000, 800])\n",
      "torch.Size([1000, 800])\n",
      "torch.Size([1000, 800])\n",
      "torch.Size([1000, 800])\n",
      "torch.Size([1000, 800])\n",
      "torch.Size([1000, 800])\n",
      "torch.Size([1000, 800])\n",
      "torch.Size([1000, 800])\n",
      "torch.Size([1000, 800])\n",
      "torch.Size([1000, 800])\n",
      "torch.Size([1000, 800])\n",
      "torch.Size([1000, 800])\n",
      "torch.Size([1000, 800])\n",
      "torch.Size([1000, 800])\n",
      "torch.Size([1000, 800])\n",
      "torch.Size([1000, 800])\n",
      "torch.Size([1000, 800])\n",
      "torch.Size([1000, 800])\n",
      "torch.Size([1000, 800])\n",
      "torch.Size([1000, 800])\n",
      "torch.Size([1000, 800])\n",
      "torch.Size([1000, 800])\n",
      "torch.Size([1000, 800])\n",
      "torch.Size([1000, 800])\n",
      "torch.Size([1000, 800])\n",
      "torch.Size([1000, 800])\n",
      "torch.Size([1000, 800])\n",
      "torch.Size([1000, 800])\n",
      "torch.Size([1000, 800])\n",
      "torch.Size([1000, 800])\n",
      "torch.Size([1000, 800])\n",
      "torch.Size([1000, 800])\n",
      "torch.Size([1000, 800])\n",
      "torch.Size([1000, 800])\n",
      "torch.Size([1000, 800])\n",
      "torch.Size([1000, 800])\n",
      "torch.Size([1000, 800])\n",
      "torch.Size([1000, 800])\n",
      "torch.Size([1000, 800])\n",
      "torch.Size([1000, 800])\n",
      "torch.Size([1000, 800])\n",
      "torch.Size([1000, 800])\n",
      "torch.Size([1000, 800])\n",
      "torch.Size([1000, 800])\n",
      "torch.Size([1000, 800])\n",
      "torch.Size([1000, 800])\n",
      "torch.Size([1000, 800])\n",
      "torch.Size([1000, 800])\n",
      "torch.Size([1000, 800])\n",
      "torch.Size([1000, 800])\n",
      "torch.Size([1000, 800])\n",
      "torch.Size([1000, 800])\n",
      "torch.Size([1000, 800])\n",
      "torch.Size([1000, 800])\n",
      "torch.Size([1000, 800])\n",
      "torch.Size([1000, 800])\n",
      "torch.Size([1000, 800])\n",
      "torch.Size([1000, 800])\n",
      "torch.Size([1000, 800])\n",
      "torch.Size([1000, 800])\n",
      "torch.Size([1000, 800])\n",
      "torch.Size([1000, 800])\n",
      "torch.Size([1000, 800])\n",
      "torch.Size([1000, 800])\n",
      "torch.Size([1000, 800])\n",
      "torch.Size([1000, 800])\n",
      "torch.Size([1000, 800])\n",
      "torch.Size([1000, 800])\n",
      "torch.Size([1000, 800])\n",
      "torch.Size([1000, 800])\n",
      "torch.Size([1000, 800])\n",
      "torch.Size([1000, 800])\n",
      "torch.Size([1000, 800])\n",
      "torch.Size([1000, 800])\n",
      "torch.Size([1000, 800])\n",
      "torch.Size([1000, 800])\n",
      "torch.Size([1000, 800])\n",
      "torch.Size([1000, 800])\n",
      "torch.Size([1000, 800])\n",
      "torch.Size([1000, 800])\n",
      "torch.Size([1000, 800])\n",
      "torch.Size([1000, 800])\n",
      "torch.Size([1000, 800])\n",
      "torch.Size([1000, 800])\n",
      "torch.Size([1000, 800])\n",
      "torch.Size([1000, 800])\n",
      "torch.Size([1000, 800])\n",
      "torch.Size([1000, 800])\n",
      "torch.Size([1000, 800])\n",
      "torch.Size([1000, 800])\n",
      "torch.Size([1000, 800])\n",
      "torch.Size([1000, 800])\n",
      "torch.Size([1000, 800])\n",
      "torch.Size([1000, 800])\n",
      "torch.Size([1000, 800])\n",
      "torch.Size([1000, 800])\n",
      "torch.Size([1000, 800])\n",
      "torch.Size([1000, 800])\n",
      "torch.Size([1000, 800])\n",
      "torch.Size([1000, 800])\n",
      "torch.Size([1000, 800])\n",
      "torch.Size([1000, 800])\n",
      "torch.Size([1000, 800])\n",
      "torch.Size([1000, 800])\n",
      "torch.Size([1000, 800])\n",
      "torch.Size([1000, 800])\n",
      "torch.Size([1000, 800])\n",
      "torch.Size([1000, 800])\n",
      "torch.Size([1000, 800])\n",
      "torch.Size([1000, 800])\n",
      "torch.Size([1000, 800])\n",
      "torch.Size([1000, 800])\n",
      "torch.Size([1000, 800])\n",
      "torch.Size([1000, 800])\n",
      "torch.Size([1000, 800])\n",
      "torch.Size([1000, 800])\n",
      "torch.Size([1000, 800])\n",
      "torch.Size([1000, 800])\n",
      "torch.Size([1000, 800])\n",
      "torch.Size([1000, 800])\n",
      "torch.Size([1000, 800])\n",
      "torch.Size([1000, 800])\n",
      "torch.Size([1000, 800])\n",
      "torch.Size([1000, 800])\n",
      "torch.Size([1000, 800])\n",
      "torch.Size([1000, 800])\n",
      "torch.Size([1000, 800])\n",
      "torch.Size([1000, 800])\n",
      "torch.Size([1000, 800])\n",
      "torch.Size([1000, 800])\n",
      "torch.Size([1000, 800])\n",
      "torch.Size([1000, 800])\n",
      "torch.Size([1000, 800])\n",
      "torch.Size([1000, 800])\n",
      "torch.Size([1000, 800])\n",
      "torch.Size([1000, 800])\n",
      "torch.Size([1000, 800])\n",
      "torch.Size([1000, 800])\n",
      "torch.Size([1000, 800])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from sklearn import metrics\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "class NN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_dim):\n",
    "        super(NN, self).__init__()\n",
    "        self.bn = nn.BatchNorm1d(input_size)\n",
    "        self.fc1 = nn.Linear(4, hidden_dim)\n",
    "        self.fc2 = nn.Linear(input_size//4*hidden_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        N = x.shape[0]\n",
    "        x = self.bn(x)\n",
    "        orig_features = x[:, :200].unsqueeze(2) # (N, 200, 1)\n",
    "        vc = x[:, 200:400].unsqueeze(2) # (N, 200, 1)\n",
    "        sum = x[:, 400:600].unsqueeze(2) # (N, 200, 1)\n",
    "        binary = x[:, 600:].unsqueeze(2) # (N, 200, 1)\n",
    "        x = torch.cat([orig_features, vc, sum, binary], dim = 2) # (N, 200, 2)\n",
    "        x = F.relu(self.fc1(x)).reshape(N, -1) # (N, 200*hidden_dim)\n",
    "        return torch.sigmoid(self.fc2(x)).view(-1)\n",
    "\n",
    "DEVICE = torch.device(\"mps\")\n",
    "\n",
    "model = NN(input_size=800, hidden_dim=200).to(DEVICE)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "loss_fn = nn.BCELoss()\n",
    "train_loader = DataLoader(train_ds, batch_size=1000, shuffle=True)\n",
    "val_loader = DataLoader(val_ds, batch_size=1000)\n",
    "test_loader = DataLoader(test_ds, batch_size=1000)\n",
    "\n",
    "def get_predictions(loader, model, device):\n",
    "    model.eval()\n",
    "    saved_preds = []\n",
    "    true_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x,y in loader:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            scores = model(x)\n",
    "            saved_preds += scores.tolist()\n",
    "            true_labels += y.tolist()\n",
    "\n",
    "    model.train()\n",
    "    return saved_preds, true_labels\n",
    "\n",
    "for epoch in range(20):\n",
    "    probabilities, true = get_predictions(val_loader, model, device=DEVICE)\n",
    "    print(f\"VALIDATION ROC: {metrics.roc_auc_score(true, probabilities)}\")\n",
    "\n",
    "    for batch_idx, (data, targets) in enumerate(train_loader):\n",
    "        data = data.to(DEVICE)\n",
    "        targets = targets.to(DEVICE)\n",
    "\n",
    "        # forward\n",
    "        scores = model(data)\n",
    "        loss = loss_fn(scores, targets)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "def get_submission(model, loader, test_ids, device):\n",
    "    all_preds = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for x,y in loader:\n",
    "            print(x.shape)\n",
    "            x = x.to(device)\n",
    "            score = model(x)\n",
    "            prediction = score.float()\n",
    "            all_preds += prediction.tolist()\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        \"ID_code\" : test_ids.values,\n",
    "        \"target\" : np.array(all_preds)\n",
    "    })\n",
    "\n",
    "    df.to_csv(\"/Users/svenschnydrig/My Drive/Data Science Project - Team D/submission/various_submissions/fe1_binary_sum_vc.csv\", index=False)\n",
    "\n",
    "get_submission(model, test_loader, test_ids, DEVICE)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sources\n",
    "- https://www.youtube.com/watch?v=MOnk75_8b9M&t=2995s\n",
    "- https://pytorch.org/tutorials/beginner/basics/buildmodel_tutorial.html\n",
    "- https://www.amazon.de/Sebastian-Raschka/dp/1801819319/ref=sr_1_1_sspa?crid=1LIHNO6T7C2AI&keywords=pylampe&qid=1671791736&sprefix=pytorch%2Caps%2C99&sr=8-1-spons&sp_csd=d2lkZ2V0TmFtZT1zcF9hdGY&psc=1\n",
    "- https://medium.com/analytics-vidhya/santander-customer-transaction-prediction-an-end-to-end-machine-learning-project-2cb763172f8a"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15 | packaged by conda-forge | (main, Nov 22 2022, 08:52:10) \n[Clang 14.0.6 ]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8dce8878ebb52da75faff629455dee1fd1c94f17ecb542480b5d416cf2a25925"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
