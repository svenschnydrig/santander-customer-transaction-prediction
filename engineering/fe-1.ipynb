{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":22574,"status":"ok","timestamp":1671722554982,"user":{"displayName":"Sven Schnydrig","userId":"07537939478620197257"},"user_tz":-60},"id":"sal0YzsE_fwC","outputId":"2e945396-17cc-415e-b359-3fd284601652"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["## Importing libraries\n","\n","import pandas as pd\n","import numpy as np\n","from tqdm import tqdm\n","from google.colab import drive\n","\n","drive.mount('/content/drive', force_remount=True)\n","\n","import warnings\n","warnings.filterwarnings('ignore')"]},{"cell_type":"code","source":["##### The following codes were inspired by the following kernels:\n","######### 1/ Raghuram Nandepu (\"https://medium.com/analytics-vidhya/santander-customer-transaction-prediction-an-end-to-end-machine-learning-project-2cb763172f8a\") - primarily the \"sum\" section \n","######### 2/ YAG320 (\"https://www.kaggle.com/code/yag320/list-of-fake-samples-and-public-private-lb-split/notebook\") - primarily the separation between fake and real data in the train set"],"metadata":{"id":"t9MplH5PSjrn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Lgz-moMHI8cx"},"source":["### Binary"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Q_fsJfu__kz2"},"outputs":[],"source":["## Importing data\n","\n","df_train = pd.read_csv('/content/drive/MyDrive/Data Science Project - Team D/data/raw-data/train.csv')\n","df_test = pd.read_csv('/content/drive/MyDrive/Data Science Project - Team D/data/raw-data/test.csv')\n","\n","features = [col for col in df_train.columns if col.startswith(\"var\")]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VkyYinS5AhXQ"},"outputs":[],"source":["## Fake vs. Real data \n","\n","test = df_test.drop(['ID_code'], axis=1).values\n","\n","unique_count = np.zeros_like(test)\n","\n","for feature in range(test.shape[1]):\n","    _, index, count = np.unique(test[:, feature], return_counts=True, return_index=True)\n","    unique_count[index[count == 1], feature] += 1\n","    \n","real_samples = np.argwhere(np.sum(unique_count, axis=1) > 0)[:, 0]\n","synth_samples = np.argwhere(np.sum(unique_count, axis=1) == 0)[:, 0]\n","\n","df_all = pd.concat([df_train, df_test.iloc[real_samples]])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2jMoCGSYI8cz","executionInfo":{"status":"ok","timestamp":1671567056681,"user_tz":-60,"elapsed":140227,"user":{"displayName":"Sven Schnydrig","userId":"07537939478620197257"}},"outputId":"75901295-919d-44c0-dae6-603222490ac0"},"outputs":[{"output_type":"stream","name":"stdout","text":["Training set shape after creating magic features: (200000, 402)\n","Test set shape after creating magic features: (200000, 401)\n"]}],"source":["## Magic Features - Binary\n","\n","df_all_copy = df_all.copy()\n","test_fake = df_test.iloc[synth_samples]\n","\n","for feature in features:\n","    count = df_all_copy[feature].value_counts().to_dict()\n","    df_all_copy[feature+\"_unique\"] = df_all_copy[feature].apply(lambda x: 1 if count[x] == 1 else 0).values\n","    test_fake[feature+\"_unique\"] = 0 \n","\n","df_train = df_all_copy[df_all_copy[\"ID_code\"].str.contains(\"train\")].copy()\n","test_real = df_all_copy[df_all_copy[\"ID_code\"].str.contains(\"test\")].copy()\n","test_real.drop([\"target\"], axis=1, inplace=True)\n","df_test = pd.concat([test_real, test_fake], sort=False).sort_index()\n","\n","print('Training set shape after creating magic features: {}'.format(df_train.shape))\n","print('Test set shape after creating magic features: {}'.format(df_test.shape))\n","\n","\n","df_test.to_csv(\"/content/drive/MyDrive/Data Science Project - Team D/data/fe1-binary/test_fe1_binary.csv\", index=False)\n","df_train.to_csv(\"/content/drive/MyDrive/Data Science Project - Team D/data/fe1-binary/train_fe1_binary.csv\", index=False)"]},{"cell_type":"markdown","metadata":{"id":"iQosLTLnI8c0"},"source":["### Sum"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wLkAgKXkI8c0"},"outputs":[],"source":["## Importing data\n","\n","df_train = pd.read_csv('/content/drive/MyDrive/Data Science Project - Team D/data/raw-data/train.csv')\n","df_test = pd.read_csv('/content/drive/MyDrive/Data Science Project - Team D/data/raw-data/test.csv')\n","\n","features = [col for col in df_train.columns if col.startswith(\"var\")]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fIQMiJjvI8c1"},"outputs":[],"source":["## Fake vs. Real data \n","\n","test = df_test.drop(['ID_code'], axis=1).values\n","\n","unique_count = np.zeros_like(test)\n","\n","for feature in range(test.shape[1]):\n","    _, index, count = np.unique(test[:, feature], return_counts=True, return_index=True)\n","    unique_count[index[count == 1], feature] += 1\n","    \n","real_samples = np.argwhere(np.sum(unique_count, axis=1) > 0)[:, 0]\n","synth_samples = np.argwhere(np.sum(unique_count, axis=1) == 0)[:, 0]\n","\n","df_all = pd.concat([df_train, df_test.iloc[real_samples]])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gA99e9rwI8c1","executionInfo":{"status":"ok","timestamp":1671567554693,"user_tz":-60,"elapsed":480477,"user":{"displayName":"Sven Schnydrig","userId":"07537939478620197257"}},"outputId":"2d0445d8-04e3-4fba-921c-335be9e4e055"},"outputs":[{"output_type":"stream","name":"stdout","text":["Training set shape after creating magic features: (200000, 1002)\n","Test set shape after creating magic features: (200000, 1001)\n"]}],"source":["df_all_copy = df_all.copy()\n","\n","for feature in features:\n","    temp = df_all_copy[feature].value_counts(dropna=True)\n","    df_train[feature + 'vc'] = df_train[feature].map(temp).map(lambda x: min(10, x)).astype(np.uint8)\n","    df_test[feature + 'vc'] = df_test[feature].map(temp).map(lambda x: min(10, x)).astype(np.uint8)\n","\n","for feature in features:\n","    df_train[feature + 'sum'] = ((df_train[feature] - df_all_copy[feature].mean()) * df_train[feature + 'vc'] \\\n","                                 .map(lambda x: int(x > 1))).astype(np.float32)\n","    df_test[feature + 'sum'] = ((df_test[feature] - df_all_copy[feature].mean()) * df_test[feature + 'vc'] \\\n","                                .map(lambda x: int(x > 1))).astype(np.float32) \n","\n","for feature in features:\n","    df_train[feature + 'sum2'] = ((df_train[feature]) * df_train[feature + 'vc'] \\\n","                                  .map(lambda x: int(x > 2))).astype(np.float32)\n","    df_test[feature + 'sum2'] = ((df_test[feature]) * df_test[feature + 'vc'] \\\n","                                 .map(lambda x: int(x > 2))).astype(np.float32)\n","\n","for feature in features:\n","    df_train[feature + 'sum3'] = ((df_train[feature]) * df_train[feature + 'vc'] \\\n","                                  .map(lambda x: int(x > 4))).astype(np.float32) \n","    df_test[feature + 'sum3'] = ((df_test[feature]) * df_test[feature + 'vc'] \\\n","                                 .map(lambda x: int(x > 4))).astype(np.float32)\n","\n","print('Training set shape after creating magic features: {}'.format(df_train.shape))\n","print('Test set shape after creating magic features: {}'.format(df_test.shape))\n","\n","df_train.to_csv(\"/content/drive/MyDrive/Data Science Project - Team D/data/fe1-sum/train_fe1_sum.csv\", index=False)\n","df_test.to_csv(\"/content/drive/MyDrive/Data Science Project - Team D/data/fe1-sum/test_fe1_sum.csv\", index=False)"]},{"cell_type":"markdown","source":["## Sum for LGBM"],"metadata":{"id":"RQ0ljDNQN2hR"}},{"cell_type":"code","source":["df_train = pd.read_csv('/content/drive/MyDrive/Data Science Project - Team D/data/raw-data/train.csv')\n","df_test = pd.read_csv('/content/drive/MyDrive/Data Science Project - Team D/data/raw-data/test.csv')"],"metadata":{"id":"TaWAvTlaN2H8","executionInfo":{"status":"ok","timestamp":1671722574840,"user_tz":-60,"elapsed":15469,"user":{"displayName":"Sven Schnydrig","userId":"07537939478620197257"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["## Fake vs. Real data \n","\n","test = df_test.drop(['ID_code'], axis=1).values\n","\n","unique_count = np.zeros_like(test)\n","\n","for feature in range(test.shape[1]):\n","    _, index, count = np.unique(test[:, feature], return_counts=True, return_index=True)\n","    unique_count[index[count == 1], feature] += 1\n","    \n","real_samples = np.argwhere(np.sum(unique_count, axis=1) > 0)[:, 0]\n","synth_samples = np.argwhere(np.sum(unique_count, axis=1) == 0)[:, 0]"],"metadata":{"id":"B6xL9AveJrCj","executionInfo":{"status":"ok","timestamp":1671722582620,"user_tz":-60,"elapsed":6186,"user":{"displayName":"Sven Schnydrig","userId":"07537939478620197257"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["## Training model & returning submission\n","\n","features = [col for col in df_train.columns if col.startswith('var')]\n","df_all = pd.concat([df_train, df_test.iloc[real_samples]])\n","\n","for feature in features:\n","    temp = df_all[feature].value_counts(dropna=True)\n","\n","    df_train[feature + 'vc'] = df_train[feature].map(temp).map(lambda x: min(10, x)).astype(np.uint8)\n","    df_test[feature + 'vc'] = df_test[feature].map(temp).map(lambda x: min(10, x)).astype(np.uint8)\n","\n","    df_train[feature + 'sum'] = ((df_train[feature] - df_all[feature].mean()) * df_train[feature + 'vc'] \\\n","                                 .map(lambda x: int(x > 1))).astype(np.float32)\n","    df_test[feature + 'sum'] = ((df_test[feature] - df_all[feature].mean()) * df_test[feature + 'vc'] \\\n","                                .map(lambda x: int(x > 1))).astype(np.float32) \n","\n","    df_train[feature + 'sum2'] = ((df_train[feature]) * df_train[feature + 'vc'] \\\n","                                  .map(lambda x: int(x > 2))).astype(np.float32)\n","    df_test[feature + 'sum2'] = ((df_test[feature]) * df_test[feature + 'vc'] \\\n","                                 .map(lambda x: int(x > 2))).astype(np.float32)\n","\n","    df_train[feature + 'sum3'] = ((df_train[feature]) * df_train[feature + 'vc'] \\\n","                                  .map(lambda x: int(x > 4))).astype(np.float32) \n","    df_test[feature + 'sum3'] = ((df_test[feature]) * df_test[feature + 'vc'] \\\n","                                 .map(lambda x: int(x > 4))).astype(np.float32)\n","    \n","print('Training set shape after creating magic features: {}'.format(df_train.shape))\n","print('Test set shape after creating magic features: {}'.format(df_test.shape))\n","\n","df_train.to_csv(\"/content/drive/MyDrive/Data Science Project - Team D/data/fe1-sum/train_fe1_sum_lgbm.csv\", index=False)\n","df_test.to_csv(\"/content/drive/MyDrive/Data Science Project - Team D/data/fe1-sum/test_fe1_sum_lgbm.csv\", index=False)"],"metadata":{"id":"vZIkybXVN_nk","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1671723011966,"user_tz":-60,"elapsed":427836,"user":{"displayName":"Sven Schnydrig","userId":"07537939478620197257"}},"outputId":"73145eec-7a94-444e-af73-83ce953b1aa6"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Training set shape after creating magic features: (200000, 1002)\n","Test set shape after creating magic features: (200000, 1001)\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"4MHrrnV5bkPP"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"machine_shape":"hm","provenance":[]},"gpuClass":"premium","kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.6"},"vscode":{"interpreter":{"hash":"aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"}}},"nbformat":4,"nbformat_minor":0}